[{"authors":null,"categories":null,"content":"Text about yourself\n","date":1699649391,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1699649391,"objectID":"97f05c9047c36712d48926bdd84b3d18","permalink":"https://carlhenrik.com/cdt-man-cam/author/template-user/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cdt-man-cam/author/template-user/","section":"authors","summary":"Text about yourself","tags":null,"title":"Template User","type":"authors"},{"authors":null,"categories":null,"content":"I\u0026rsquo;m a professor in the Machine Learning Group and head of the Computational and Biological Learning Lab in the Division of Information Engineering at the Department of Engineering in Cambridge. I work on machine learning and on climate change. I don\u0026rsquo;t travel professionally by air because it destroys the habitability of earth. Research\nI\u0026rsquo;m interested in the theory and practice of understanding and building systems that learn and make decisions. Humans have an exceptional ability to learn from experience, which sets them apart from current artificial intelligent (AI) systems. To understand human learning and design better AI we need principled approaches to learning and decision making based on Bayesian inference in machine learning. My interests span: probabilistic inference, reinforcement learning, approximate inference (variational and MCMC), decision making, non-parametric modeling, stochastic processes and efficient learning.\nMy first mentor was David Willshaw; I completed my MSc with Lars Kai Hansen and PhD with Geoff Hinton.\n","date":1699650600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1699650600,"objectID":"ed825193c405085a5b1d72c3d411d0b7","permalink":"https://carlhenrik.com/cdt-man-cam/author/carl-rasmussen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cdt-man-cam/author/carl-rasmussen/","section":"authors","summary":"I\u0026rsquo;m a professor in the Machine Learning Group and head of the Computational and Biological Learning Lab in the Division of Information Engineering at the Department of Engineering in Cambridge. I work on machine learning and on climate change.","tags":null,"title":"Carl Rasmussen","type":"authors"},{"authors":null,"categories":null,"content":"My name is Carl Henrik Ek and I am a Associate Professor in the Computer Laboratory at the University of Cambridge, UK. and a Docent in Machine Learning at the Royal Institute of Technology, Sweden.\nLearning is the task of associating a new phenomena to previous knowledge. Knowledge is the capability of providing structure to the environment. In the field of machine learning we try to build methods that are capable of learning from data. The fundamental aspect of learning is assumptions, being the realisation of knowledge, the science of machine learning is concerned with how to formulate assumptions into mathematics (modelling) and how to related them to observed data (inference). My research focus spans both these areas, in specific I am interested in how we can specify data efficient and interpretable assumptions that allows us to learn from small amounts of data. Most of my work is focused on Bayesian non-parametric methods and in specific Gaussian processes.\n","date":1699649391,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1699649391,"objectID":"150e7756ab670b6f8b3127b5c2b2bb0a","permalink":"https://carlhenrik.com/cdt-man-cam/author/carl-henrik-ek/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cdt-man-cam/author/carl-henrik-ek/","section":"authors","summary":"My name is Carl Henrik Ek and I am a Associate Professor in the Computer Laboratory at the University of Cambridge, UK. and a Docent in Machine Learning at the Royal Institute of Technology, Sweden.","tags":null,"title":"Carl Henrik Ek","type":"authors"},{"authors":["Carl Henrik Ek","Carl Rasmussen"],"categories":null,"content":"Bayesian optimisation (BO) is an important and commonly used technique to search for the optima of an either unknown function or a function that is very computationally expensive to evaluate. BO is extensively used in an industrial settings and are solving real and important problems.\nThe premise of BO is that we create a statistical surrogate model \\(p(f)\\) of the function that encodes our beliefs about its structure. This prior belief is then updated with the observations \\(Y=\\{y_i \\}_{i=1}^N\\) that we observe to reach a posterior belief \\(p(f\\mid Y)\\). The goal is to use this posterior belief to efficiently search for the extremum of the function. Traditionally this is done by formulating a function called an acquisition function that takes as input our current belief over of the function. The modelling and inference of the statistical model is based on sound established principles the notion of an acquisition function is less principled and more of an ad-hoc structure. While this have been shown to work well in practice it means that the decision outcome is hard to explain and justify from principle.\nIn this project we want to study the decision process with a perspective from statistical inference. We will take inspiration from the field of Probabilistic Numerics [1]. which is the study of how to interpret the computational process as statistical inference. Probabilistic Numerics formulates a computational process using the following components, a latent quantity \\(u\\) from which we want to extract a specific quantity of interest \\(\\hat{f}\\). The operation generating this quantity is called the quantity of interest operator. If the latent quantity is unknown as in BO we try to design a computational process using a information operator that queries the latent quantity and an algorithm that aims to estimate the quantity of interest from a finite This interpretation of BO implies that we are interested viewing the search problem for the minima \\(f_{min}\\) of a function as the following problem, \\[ p(f_{min}\\mid Y) = \\int p(f_{min}\\mid f)p(f\\mid Y) \\textrm{d}f. \\] The posterior distribution above is challenging to formulate. This project will focus on both understanding current methods in terms of approximations to the above posterior and as mean of developing new more principled ways of addressing the problem.\nReferences\nHennig, P., Osborne, M. A., \u0026amp; Kersting, H. P. (2022). Probabilistic numerics: computation as machine learning. ","date":1699650843,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699650843,"objectID":"f832542b557c188bd379565a1b14e125","permalink":"https://carlhenrik.com/cdt-man-cam/talk/2024_bayesopt_ek_rasmussen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cdt-man-cam/talk/2024_bayesopt_ek_rasmussen/","section":"talk","summary":"Principles of sequential decision making in Bayesian Optimisation","tags":null,"title":"Bayesian optimisation","type":"talk"}]